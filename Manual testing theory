Software testing ---> to ensure quality of developed software ---> find bugs and fix them before the product is released to the user.
Why testing? ----> provides quality assurance, removes errors at the initial stages, ensures lower maintainance cost and accuracy and consistency, BUG Free, multiple OS support.
categories of software testing ---> MANUAL TESTING, AUTOMATION TESTING.
quality control? ---> product-oriented approach to ensure successfull execution without defects and with requirements as per client.

types of manual testing ---> Blackbox testing, whitebox, unit, integration, system, acceptance.
blackbox testing ---> to analyze funtionality of software without knowing much about design or structure of software they are testing.
whitebox testing ---> requires profound knowledge of code, as it includes testing of some structural part of the application.
UNIT testing ---> testing smallest piece of code.
INTEGRATION testing ---> testing interface between units .
SYSTEM testing ----> to examine fully working of an integrated software system against user rqrmnt.
ACCEPTANCE testing ---> formal testing based on user requirement and function processing, covers end-users real-world scenarios.
acceptance ---> provides you result on basis of if product is fit to release to the market or not.

functional testing: whether application is providing all the functionalities as per reqmnts. focus on requiremnts, black box,{what does the product do} before non fucntional
usecase: helps is checking login functionality
non functonal testing: checks attributes of system or an application such as memory mgmt, performance, health of system. focus on user expectatiibs, white box {how does product do}
use case:to load dashboard in few seconds.

types of functional testing:
unit, sanity, smoke, regression, integration, usability

types of non functional testing:
documentation testing: helps in estimate testing effort rqd, test coverage.
Installation testing: testing whether working or not after installation.
performance testing: heart of testing, system performs well under any work load
    diff types of performance testing:
    -->load testing: measure response time and staying power of any application when system performing well under heavy load.
    -->stree testing: validate applications behavior when pushed beyond peak load conditions.
    -->endurance testing: tested with expected amount of load over a long period of time to find behavior of system.
    -->spike testing: to validate performance characteristics when the system under test is subjected to work load models and load volumes so
Reliability testing:whether software can perform a failure free operation for a specific period of time.
Security testing: ensures protection of its data and resources 


ALPHA testing ---> to identify bugs before releasing product to real users, type of user acceptance testing. [black box white box]
BETA testing ---> performed by real users of software in real env, type of user acceptance testing. (because not possible for testers to test for every possible edge case)
BETA -> taking feedback from user himself.

levels of manual testing ----> unit testing --> integration testing ----> system testing ----> acceptance testing.

quality control ---> set of activities for ensuring quality in products ---> aims to identify defects in finished product ---> product oriented approch.
qaulity assurance ----> set of activities for ensuring quality in process by which product are developed ---> aims to prevent defects with focus on process ---> process-oriented.

TESTBED in manual testing
---> test execution env configd for testing (software and hardware both).

procedure for manual testing:
1. understand requirements
2. categorize requirement and write test cases
3. conduct tests
4.create Bug report after test result and submit to manager


TEST CASE ---> doc contains set of conditions which are performed on software in order to verify expected functionality of the feature.

API Testing ---> application programming interface
testing apis directly and determine if they meet expectations for functionality, reliability, performance and security.
lang independent, gui independent, improved test coverage, reduces testing cost, enables faster releases.

Application has 3 categories:
1. presentation layer ==> gui testing
2. business layer ===> api testing
3. database layer


VERIFICATION: testing done without execution of code (examples, reviews, walkthroughs, demos)
VALIDATION: testing with execution of code (non functional, functional testing)

diff between bug and defect and error ?
DEFECT: variance between expected and actual. Error found after application goes into production. refers to troubles with software products with external behavior or internalfeature.
BUG: result of coding error, found in dev env before shipping to customer. (block functionality)
ERROR: mistake misunderstanding misconception on part of SDE.

Advantages of manual testing:
--> live testing: testing under similar condition when an application goes live.
--> less programming knowledge required
--> UI UX issues
--> Low cost investment, cost effective for short-term projects
--> adaptability to adapt to changes made in application

Disadvantages of manual testing:
--> time consuming
--> cant be reused --> for every change in application --> run all the test cases again and again.
--> possibility of errors (human)

is documentation really necessary in manual testing? ----
***** YES ****
--> to achieve effective software testing.
--> contains each and every minute detail.
--> document test cases ---> can estimate testing efforts -->  track and trace requirements.
___________________________________________________________________________________________________________________________________________________________________________________________

difference between manual testing and automation testing ?
manual by human and human actions ---- auto by humans with help of tools, scripts.
manual low accuracy, reliability ---- auto more reliable
manual needs more time ---- auto needs low time.
manual investment cost and roi both low ------ auto investment cost and roi both high.
manual prefered when test cases run once/twice, suitable for exploratory, usability and adhoc testing ------- auto for regression testing, performance testing, load testing.
manual allows human obs to find glitches, improves customer experience --------- auto no human obs involved, no guarantee of + customer experience.

Use manual testing over automation testing when,
--> short time projects
--> when performing adhoc testing. (no fixed approach/planning )
--> when performing exploratory testing
--> when performing usability testing


Phases in SOFTWARE TESTING LIFE CYLCE [STLC]
1. reqmnt analysis
2. test planning
3. test case development
4. test env setup
5. test execution
6. test cycle closure


what makes a good test engineer ?
--> test to break attitude --> strong desire for quality, keen attention to detail
--> Tact and diplomacy roles needed to maintain cooperative relation with rest of devs 
--> ability to communicate with both technicalppl and nontech ppl.
--> ability to judge situations ---> make imp decisions ---> to test highrisk areas of an application when time is ltd.
--> experience already in the field.
__________________________________________________________________________________________________________________________________________________________________________________________

***REGRESSION Testing:
to ensure small change in one part of system doesnt break the functionality elsewhere in the system.
when adding new payment type to shopping webiste, you have to rerun the old test to ensure that new code hasnt created any new defects/ reintroduced any old defects.

test the software ---> add new feature ---> test whether new feature brought any old bug or created new bugs --> this test : "REGRESSION TESTING"


diff between system testing and integration testing?

SYSTEM TESING: test sftware as a whole to check if system is compilant with user requirements
involves both functional and non functional testing.
high level testing performed after integration testing.
INTEGRATION TESTING: test interface between modules of software application, only functional testing performed, lowlevel testing after unit testing.

DEFECT LIFE CYCLE: 
process in which a defect go thru various phases during its lifetime. start whendefect found, ends when defect is closed, ensuring its not resurface.

1.bug identified
2. assigned to a developer
3. bug status = active
4. if dev feels bug not genuine --> bug status = rejected
    if dev feels bug expected to be fixed in next releases --> bug status = deferred
5. tester test after dev fixed the bug, if bug not found then status = verified, if exists status = reopen (again go back to active state)
6. status = verified ----> bug closed.

new ---> assign ----> active (-->rejected or deferred) ----> test (-->reopen --> active) ----> verified ---> closed

algo: 
new bug;
assign to dev;
status = active {
    if(bug not genuine) rejected;
    if(bug fix in next release) deferred;
}
dev fixed the bug and pushed to tester;
tester testing the bug;
if(test==bug) {
  reopen() --> active;
}
else status = verified
bug closed



TEST HARNESS: 
--> gather software and test info.
--> this info was arrngd to test program unit by running it under changing conditions like stress, load, data-driven and monitoring its behaivor and outputs.
--> Harness has two parts ---> test execution engine and test script repo.

TEST CLOSURE:
doc which gives summary of all tests conducted during SDLC --> gives detailed analysis of bugs removed and errors found.

Positive and negative testing diff:
POSITIVE testing: determines that your application works as expected. if error found test fails. tester always check for valid set of values only.
NEGATIVE testing: determines whether your application can handle invalid input or unexpected user behavior

CRITICAL BUG: tendancy to affect majority functionality of given application.

PESTICIDE PARADOX:
if same tests repeated again and again, eventually these test cases will no longer find new bugs.
how to overcome: write whole new set of test cases, prepate new test cases and add them to existing test cases.


DEFECT CASCADING:
process of triggering other defects in application
--> when defect goes unnoticed while testing, it invokes other defects, leading to incorrect ouput.
--> multiple defects crop up in later stages of development.

QUALITY SOFTWARE: 
bug-free, delivered on time, within budget, meets requirments, maintainable.
quailty is subjective, each client will have their own slant for quality

BLACK BOX TESTING and types?
spec-based testing, analyses functionality of software without knowing much about the internal structure of item.
blackbox TECHNIQUES:
-->Equivalence Partitioning
-->Boundary value analysis
-->Decision table based technique
-->cause-effect graphing
-->use case testing

WHITE BOX TESTING and types?
structure based testing, have knwoledge about internl structural part of application
whitebox TECHNIQUES:
-->Statement Coverage
-->Decision Coverage
-->Condition Coverage
-->Multiple Condition Coverage

what are experience based testing?
discovery, investigation, learning. constant study and analysis of product required. apply your skills and experience to dev test strategy and test cases to perform necessary testing.
techniques:
--> Exploratory 
--> Error guessing
__________________________________________________________________________________________________________________________________________________________________________________________

TOP DOWN Approach:
testing from top to bottom
high level modules tested first and then low level modules.
BOTTOM UP Approach:
base levels to high-up levels.
first low level modules are tested first and afterward high-level state models.


diff between smoke testing and sanity testing:

SMOKE TESTING: test executed on initial builds of product
motive --> to measure stability of initial builds
---->involves documentation and scripting work
test coverage --> shallow and wide approach
performed by? --> developers or testers

SANITY TESTING: tests done on builds that have passed smokeand regression tests
motive --> to evaluate rationality and originality of functionalities of builds
----> doesnt emphasize on any sort of documentation
test coverage -->  narrow and deep approach
executed by testers.


diff between static testing and dynamic testing:

STATIC TESTING:
--> white box technique
--> performed at early stages of SDLC to identify imperfections.
--> implemented at verification stage
--> before code deployment
--> code error detection and execution of program not really a concern

DYNAMIC TESTING:
--> execution of code and done at later stages of SDLC
--> validates and approves output with expected results
--> starts during validation stage.
--> performed after code deployment
--> execution of code necessary
__________________________________________________________________________________________________________________________________________________________________________________________
Scenario based questions:

deciding when to stop testing is difficult, common factors to decide are:
--> considering deadlines
--> testcases completed with certain pass %
--> when budget exhausted
--> coverage of code has reached specific point
--> when bug rate falls below a certain level
--> when beta or alpha testing period ends.


What if software is so buggy, it cant really be tested?
in such situation
--> report to higher authority, with documentation proof and inform.

how do you know code has met specifications?
code is good if its bugfree and readable and maintainable
--> tools like traceability matrix to which ensures the requirements are mapped to test cases.
--> when execution of all TC's finishes with a success,  it indicates that code has met the requiement.

When you will choose automatic testing over manual testing:
when there are repetitive changes
execution to be done in standard env
when there is less time to complete testing
lot of code that is to be tested multiple times
when reports are required for execution.


SOFTWARE CONFIGURATION MANAGEMENT (scm)
set of processes policies and tools that organise control cooridnate and track:
code, doc, problems, change request, designs and tools, compilers and libraries.


can we do system testing at any stage?
NO , must start only if all units are in place and working properly. usually happens before UAT( user acceptance testing)

BEST practices to follow when writing test cases:

--> prioritize test cases based on project timelines and risk
--> remember 80/20 rule --> 20% of your test should cover 80% of your application.
--> dont try test cases in one attempt
--> make sure test cases are modular
--> keep them simple
--> always kepp end users requiremnet in mind
--> actively use test mgmt tool
--> monitor test cases regularly.

automation testing cant replace manual testing****

SDLC -- >
requirement --> analysis --> design ---> coding --> testing --> deployment--> .....
